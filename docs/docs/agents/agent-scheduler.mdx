---
title: Scheduler Agent
---

## Purpose

Orchestrate and schedule all agents in the system by managing periodic tasks, pipeline workflows, and job dependencies. The scheduler is **completely generic** and **database-driven**—it has no hardcoded agent configurations. Admins create schedules, job templates, and pipelines through the admin interface, and the scheduler dynamically discovers agents and executes jobs based on database configurations.

The scheduler enqueues jobs for independent agent execution based on schedules stored in the database (cron, interval, or on-demand). Agents execute independently as BullMQ workers, reading from and writing to the database. The scheduler does not wait for agent completion but tracks job status through the database.

**Key Responsibilities**:
- Dynamically discover and register agents from the database (no hardcoded agent list)
- Execute schedules created by admins (stored in `Schedule` table)
- Expand job templates with parameter expansion (e.g., one schedule → many jobs for many tickers)
- Orchestrate multi-agent pipelines with job dependencies (stored in `Pipeline` table)
- Manage different schedule types: cron expressions, intervals, and on-demand triggers
- Create and enqueue jobs to the job queue with proper dependencies and priorities
- Monitor job status through database queries
- Handle retries for failed jobs
- Support manual triggers via API or admin interface

## Inputs

```typescript
interface SchedulerInput {
  // For scheduled tasks (from database)
  scheduleId?: string; // ID of schedule from Schedule table
  
  // For manual/on-demand triggers
  trigger?: {
    type: 'manual' | 'event-driven',
    jobTemplateId?: string; // Reference to JobTemplate in database
    agentId?: string; // Agent ID (must exist in AgentRegistry)
    params?: Record<string, any>; // Parameters to pass to job (merged with template params)
    expansionParams?: {
      // Parameter expansion configuration
      expandTickers?: {
        scope: 'all' | 'active' | string[]; // Which tickers to expand
        batchSize?: number;
        staggerDelay?: number;
      };
      expandUsers?: {
        userIds?: string[]; // Specific users, or all if empty
        tickerIds?: string[]; // Filter by ticker subscriptions
      };
    };
  },
  
  // For pipeline execution
  pipelineId?: string; // ID of pipeline from Pipeline table
  pipelineParams?: Record<string, any>; // Parameters for pipeline execution
}
```

## Database Schema

The scheduler uses the following database tables (stored in PostgreSQL):

### `AgentRegistry`
Stores metadata about all available agents in the system. Agents register themselves here.

```typescript
{
  id: string; // Agent ID (e.g., 'query-strategy', 'data-collection')
  name: string; // Human-readable name
  description: string;
  version: string;
  inputSchema: object; // JSON Schema or Zod schema (JSON representation)
  parameterTypes: {
    // Defines what parameters this agent accepts and how they can be expanded
    ticker?: {
      required: boolean;
      expandable: boolean; // If true, scheduler can expand this to multiple jobs
      scope: 'all' | 'active' | 'custom'; // Default scope for expansion
    };
    userId?: {
      required: boolean;
      expandable: boolean;
    };
    // ... other parameter types
  };
  enabled: boolean;
  createdAt: Date;
  updatedAt: Date;
}
```

### `Schedule`
Stores all schedules created by admins. No hardcoded schedules.

```typescript
{
  id: string;
  name: string;
  description?: string;
  type: 'cron' | 'interval' | 'on-demand';
  cronExpression?: string; // For cron type
  interval?: number; // For interval type (milliseconds)
  timezone?: string; // Default: 'America/New_York'
  enabled: boolean;
  jobTemplateId: string; // References JobTemplate
  expansionConfig?: {
    // How to expand parameters when creating jobs
    expandTickers?: {
      scope: 'all' | 'active' | string[]; // Which tickers to expand
      batchSize?: number; // Number of tickers per batch
      staggerDelay?: number; // Delay between batches (ms)
    };
    expandUsers?: {
      userIds?: string[]; // Specific users, or all if empty
      tickerIds?: string[]; // Filter by ticker subscriptions
    };
  };
  priority?: number; // Job priority (default: 0)
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `JobTemplate`
Defines reusable job templates that can be used by schedules or triggered manually.

```typescript
{
  id: string;
  name: string;
  description?: string;
  agentId: string; // References AgentRegistry
  defaultParams: object; // Default parameters (JSON)
  requiredParams?: string[]; // Parameter names that must be provided
  expansionParams?: string[]; // Parameter names that can be expanded (e.g., ['ticker', 'userId'])
  retryConfig?: {
    maxRetries: number;
    backoff: 'exponential' | 'linear' | 'fixed';
    delay: number;
  };
  timeout?: number; // Job timeout in milliseconds
  priority?: number; // Default job priority
  enabled: boolean;
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `Pipeline`
Defines multi-agent workflows with dependencies.

```typescript
{
  id: string;
  name: string;
  description?: string;
  steps: Array<{
    stepId: string; // Unique ID within pipeline
    jobTemplateId: string; // References JobTemplate
    dependsOn?: string[]; // Step IDs this step depends on
    parallel: boolean; // Can run in parallel with other steps
    condition?: {
      // Optional condition for step execution
      type: 'data-freshness' | 'custom';
      config: object;
    };
    retryConfig?: {
      maxRetries: number;
      backoff: 'exponential' | 'linear' | 'fixed';
      delay: number;
    };
    timeout?: number;
  }>;
  expansionConfig?: {
    // How to expand parameters for pipeline execution
    expandTickers?: {
      scope: 'all' | 'active' | string[];
      batchSize?: number;
      staggerDelay?: number;
    };
    expandUsers?: {
      userIds?: string[];
      tickerIds?: string[];
    };
  };
  enabled: boolean;
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `ScheduleExecution`
Tracks schedule execution history.

```typescript
{
  id: string;
  scheduleId: string; // References Schedule
  executionTime: Date;
  status: 'success' | 'partial' | 'failed';
  jobsCreated: number;
  jobsEnqueued: number;
  errors?: Array<{
    message: string;
    timestamp: Date;
  }>;
  metadata?: object; // Additional execution metadata
}
```

## Configurations

This is the minimal configuration for the scheduler agent. It is stored in the `AgentConfig` table, key: `scheduler`.

```typescript
{
  // Global scheduler settings
  scheduler: {
    enabled: true,
    timezone: 'America/New_York', // Default timezone for schedules
    checkInterval: 60000 // How often to check for due schedules (ms)
  },
  
  // Job queue configuration
  queue: {
    provider: 'bullmq',
    redisUrl: string,
    concurrency: 5, // Default concurrency for queue workers
    defaultJobOptions: {
      attempts: 3,
      backoff: { type: 'exponential', delay: 5000 }
    }
  },
  
  // Default expansion settings (can be overridden per schedule)
  defaultExpansion: {
    tickerBatchSize: 10,
    tickerStaggerDelay: 1000,
    userBatchSize: 20,
    userStaggerDelay: 500
  },
  
  // Monitoring and notifications
  monitoring: {
    enabled: true,
    notifyOnFailure: true,
    notificationChannels: ['email', 'slack'] // Optional
  }
}
```

**Note**: All agent-specific schedules, job templates, and pipelines are stored in the database tables above, not in the scheduler configuration. The scheduler dynamically loads and executes them.

## Outputs

```typescript
{
  agentId: 'scheduler',
  executionId: string,
  timestamp: Date,
  executionTime: number,
  triggerType: 'cron' | 'interval' | 'manual' | 'event-driven',
  schedulesExecuted?: Array<{
    scheduleId: string,
    scheduleName: string,
    jobTemplateId: string,
    jobTemplateName: string,
    agentId: string,
    jobsCreated: number,
    parametersExpanded?: {
      tickers?: number, // Number of tickers expanded
      users?: number, // Number of users expanded
      totalCombinations?: number // Total expanded parameter sets
    },
    batchesProcessed?: number, // For batched processing
    status: 'success' | 'partial' | 'failed'
  }>,
  jobsEnqueued: Array<{
    jobId: string,
    agentId: string,
    jobTemplateId?: string, // If created from template
    scheduleId?: string, // If part of a scheduled task
    pipelineId?: string, // If part of a pipeline
    pipelineStepId?: string, // If part of a pipeline step
    status: 'pending' | 'running' | 'completed' | 'failed',
    priority: number,
    enqueuedAt: Date,
    dependencies?: string[], // Job IDs this job depends on
    params?: Record<string, any> // Agent-specific parameters (merged from template + trigger)
  }>,
  pipelinesExecuted?: Array<{
    pipelineId: string,
    pipelineName: string,
    status: 'initiated' | 'in-progress' | 'completed' | 'partial',
    parametersExpanded?: {
      tickers?: number,
      users?: number,
      totalCombinations?: number
    },
    stepsExecuted: Array<{
      stepId: string,
      jobTemplateId: string,
      jobsCreated: number,
      status: 'pending' | 'running' | 'completed' | 'failed'
    }>,
    conditionalSteps?: {
      dataFreshnessChecks?: number,
      dataCollectionJobsTriggered?: number,
      freshDataCount?: number,
      staleDataCount?: number
    },
    metadata: {
      totalJobsCreated: number,
      totalSteps: number,
      completedSteps: number
    }
  }>,
  errors?: Array<{
    scheduleId?: string,
    jobTemplateId?: string,
    pipelineId?: string,
    agentId?: string,
    error: string,
    timestamp: Date
  }>
}
```

**Note**: The scheduler returns orchestration results immediately after enqueuing jobs. Actual job and pipeline status is tracked in the database (`ScheduleExecution` table) and can be queried separately. Agents update their job status in the database as they execute independently.

## Process

1. **Initialize**: 
   - Load scheduler config from database (`AgentConfig` table, key: `scheduler`)
   - Connect to job queue (BullMQ with Redis)
   - Load all enabled schedules from `Schedule` table
   - Load all enabled job templates from `JobTemplate` table
   - Load all enabled pipelines from `Pipeline` table
   - Load agent registry from `AgentRegistry` table (discover available agents)
   - Initialize cron scheduler for all cron-based schedules
   - Initialize interval timers for all interval-based schedules
   - Initialize job dependency tracking
   - Initialize ticker/identifier cache (optional, for performance when processing many tickers)

2. **Agent Discovery**:

   - Query `AgentRegistry` table for all enabled agents
   - Validate that agents referenced in schedules, job templates, and pipelines exist in registry
   - Load agent input schemas to validate job parameters
   - Cache agent metadata for performance

3. **Schedule Execution** (triggered by cron, interval, manual, or event):

   The scheduler handles different types of schedules from the database:

   **3a. Cron-based Schedules**:
   - Monitor all cron expressions from `Schedule` table where `type = 'cron'` and `enabled = true`
   - When cron expression matches current time, execute the corresponding schedule
   - Load the associated `JobTemplate` from the schedule's `jobTemplateId`
   - Execute job creation with parameter expansion (see section 4)

   **3b. Interval-based Schedules**:
   - Monitor interval timers for schedules where `type = 'interval'` and `enabled = true`
   - When interval elapses, execute the corresponding schedule
   - Load the associated `JobTemplate` and execute job creation

   **3c. On-demand Triggers**:
   - Handle manual triggers via API or admin interface
   - Handle event-driven triggers (e.g., entity graph updated → trigger query generation)
   - Execute immediately when triggered
   - Can reference a `JobTemplate` directly or provide ad-hoc parameters

4. **Parameter Expansion** (for agents that require expansion):

   The scheduler expands parameters based on `expansionConfig` in schedules or `expansionParams` in triggers:

   **4a. Ticker Expansion**:
   - If `expandTickers` is configured:
     - Load tickers/identifiers from database based on `scope`:
       - `'all'`: All tickers in the `Ticker` table where `enabled = true`
       - `'active'`: Only tickers with active user subscriptions (from `UserTicker` table)
       - `string[]`: Specific ticker identifiers provided in configuration
     - Filter by enabled/active status
     - Group tickers into batches based on `batchSize` configuration
     - Apply stagger delays between batches to avoid rate limits
   - For each ticker, create a separate job with `ticker` parameter set

   **4b. User Expansion**:
   - If `expandUsers` is configured:
     - Load users from database based on filters:
       - If `userIds` provided: Only those specific users
       - If `tickerIds` provided: Users subscribed to those tickers
       - Otherwise: All active users
     - Group users into batches if needed
   - For each user, create a separate job with `userId` parameter set

   **4c. Combined Expansion** (e.g., user-ticker pairs):
   - Expand both users and tickers
   - Create jobs for each user-ticker combination
   - Example: 10 users × 5 tickers = 50 jobs

5. **Job Creation** (from JobTemplate):

   For each schedule execution or manual trigger:

   **5a. Load JobTemplate**:
   - Load the `JobTemplate` referenced by the schedule or trigger
   - Validate that the template's `agentId` exists in `AgentRegistry`
   - Merge default parameters from template with trigger parameters

   **5b. Parameter Expansion**:
   - Check if any parameters in `expansionParams` need expansion
   - If ticker expansion needed: Expand to multiple tickers (see 4a)
   - If user expansion needed: Expand to multiple users (see 4b)
   - If both: Expand to user-ticker combinations (see 4c)

   **5c. Create Jobs**:
   - For each expanded parameter set (or single set if no expansion):
     - Create job with merged parameters (template defaults + trigger params + expanded params)
     - Set job priority from schedule or template
     - Set retry config from template or schedule
     - Set timeout from template
     - Enqueue to job queue
   - Track job creation in `ScheduleExecution` table

6. **Pipeline Orchestration**:

   For pipeline execution (from `Pipeline` table):

   **6a. Load Pipeline**:
   - Load pipeline definition from `Pipeline` table
   - Validate all referenced `JobTemplate` IDs exist
   - Validate all referenced agents exist in `AgentRegistry`

   **6b. Parameter Expansion** (if configured):
   - Apply `expansionConfig` from pipeline (same as schedule expansion)
   - Expand tickers, users, or both as needed

   **6c. Build Dependency Graph**:
   - Parse pipeline `steps` to build dependency graph
   - Identify steps that can run in parallel (`parallel: true`)
   - Identify steps that must run sequentially (via `dependsOn`)

   **6d. Execute Pipeline Steps**:
   - For each expanded parameter set (e.g., each user-ticker pair):
     - Create jobs for all pipeline steps
     - Set job dependencies based on `dependsOn` relationships
     - Handle conditional steps (e.g., data freshness checks)
     - Enqueue jobs with proper dependency chain
   - Track pipeline execution in database

   **6e. Conditional Steps**:
   - If step has `condition.type = 'data-freshness'`:
     - Check timestamp of latest collected data in database
     - If data is stale, enqueue a high-priority Data Collection job first
     - Set dependency so pipeline step waits for data collection
   - Custom conditions can be implemented via plugins

7. **Independent Agent Execution**:

   - Agents do not receive data from other agents
   - Agents read from and write to database independently
   - Agents execute on their own schedule when jobs are available
   - No direct agent-to-agent communication
   - All coordination through database state

8. **Job Dependencies & Status Tracking**:

   - Track job status in database (pending, running, completed, failed)
   - Use job dependencies (BullMQ) to ensure proper sequencing (for pipelines)
   - Monitor job progress by querying database for job statuses
   - Handle failed jobs with retry logic (exponential backoff from template/schedule config)

9. **Parallelization**:

   - **Expanded Parameter Jobs** (e.g., per-ticker, per-user):
     - Multiple expanded items processed in parallel (controlled by `batchSize` in expansion config)
     - Jobs for different parameter sets execute concurrently across queue workers
     - Example: 100 tickers with `batchSize: 20` = 5 batches, each batch processes 20 tickers in parallel
     - Stagger delays between batches prevent rate limiting and resource exhaustion
   
   - **Single Parameter Jobs** (no expansion):
     - Single job processes all data in aggregate
     - Can run concurrently with other agent jobs
   
   - **Pipeline Steps**:
     - Steps with `parallel: true` can run concurrently
     - Steps with `dependsOn` run sequentially after dependencies complete
     - Multiple parameter expansions (e.g., user-ticker pairs) can be processed in parallel
     - Example: Newsletter pipeline with 10 users × 5 tickers = 50 parallel pipeline executions

10. **Scalability Considerations**:

   - **Large Parameter Sets**: When processing hundreds of tickers/users:
     - Batch processing prevents queue overload
     - Stagger delays prevent API rate limits
     - Jobs are distributed across multiple queue workers
     - Failed jobs for one parameter set don't block others
     - Partial failures are handled gracefully (continue with remaining items)
   
   - **Dynamic Management**:
     - New tickers/users are automatically included in next scheduled run (if scope includes them)
     - Disabled tickers/users are excluded from processing
     - Priority can be set per schedule/template to influence job priority
   
   - **Resource Management**:
     - `batchSize` in expansion config controls concurrent job execution
     - Queue concurrency settings limit total parallel workers
     - Memory and CPU usage scales with batch size and concurrency

11. **Error Handling**:

   - Failed jobs are retried with exponential backoff (configured in `JobTemplate` or `Schedule`)
   - Job dependencies can timeout if upstream jobs fail repeatedly
   - Log errors to database for monitoring (`ScheduleExecution` table)
   - Notify on critical failures (via configured notification channels)
   - Dead-letter queue for jobs that exceed max retries
   - Pipeline continues with partial results if some jobs fail (non-blocking)
   - Failed jobs for one schedule/template don't block others

12. **Metrics Collection**:

   - Agents write metrics to database upon completion
   - Scheduler queries database for job and pipeline status
   - Track schedule execution history in `ScheduleExecution` table
   - Learning Agent reads metrics from database independently (runs on its own schedule)

13. **Return**: Orchestration results (job IDs, status, metadata for all scheduled agents, including expanded parameter counts)

## Admin Interface Usage

Admins can create and manage schedules, job templates, and pipelines through the admin interface. The scheduler dynamically loads and executes these configurations without any code changes.

### Creating a Schedule

1. **Create a JobTemplate** (if it doesn't exist):
   - Navigate to `/admin/scheduler/job-templates`
   - Click "Create Job Template"
   - Select an agent from `AgentRegistry`
   - Define default parameters
   - Configure retry settings and timeout
   - Save the template

2. **Create a Schedule**:
   - Navigate to `/admin/scheduler/schedules`
   - Click "Create Schedule"
   - Select a `JobTemplate`
   - Choose schedule type (cron, interval, or on-demand)
   - Configure schedule timing (cron expression or interval)
   - Configure parameter expansion (if needed):
     - Enable ticker expansion: Select scope (all, active, or specific tickers)
     - Enable user expansion: Select users or filter by ticker subscriptions
     - Set batch size and stagger delay
   - Save the schedule

### Creating a Pipeline

1. **Ensure JobTemplates exist** for all pipeline steps

2. **Create a Pipeline**:
   - Navigate to `/admin/scheduler/pipelines`
   - Click "Create Pipeline"
   - Add pipeline steps:
     - Select a `JobTemplate` for each step
     - Define step dependencies (`dependsOn`)
     - Configure parallel execution (`parallel: true/false`)
     - Add conditional logic (e.g., data freshness checks)
   - Configure parameter expansion (if needed)
   - Save the pipeline

3. **Create a Schedule for the Pipeline**:
   - Create a schedule that references the pipeline (instead of a single job template)
   - The scheduler will execute the pipeline when the schedule triggers

### Example: Query Strategy Agent Schedule

**JobTemplate**:
- `id`: "query-strategy-entity-graph"
- `agentId`: "query-strategy"
- `defaultParams`: `{ "refreshEntityGraph": true }`
- `expansionParams`: `["ticker"]`

**Schedule**:
- `name`: "Weekly Entity Graph Refresh"
- `type`: "cron"
- `cronExpression`: "0 2 * * 0" (Sunday 2 AM)
- `jobTemplateId`: "query-strategy-entity-graph"
- `expansionConfig.expandTickers`: `{ scope: "all", batchSize: 10, staggerDelay: 1000 }`

**Result**: Every Sunday at 2 AM, the scheduler expands to all tickers and creates 100 jobs (one per ticker) that process in batches of 10.

## Sequence Diagram

<Mermaid
  chart="
flowchart TD
    subgraph Admin [Admin Interface]
        A1[Admin Creates<br/>JobTemplate]
        A2[Admin Creates<br/>Schedule]
        A3[Admin Creates<br/>Pipeline]
    end
    
    A1 --> DB1[(Database<br/>JobTemplate Table)]
    A2 --> DB2[(Database<br/>Schedule Table)]
    A3 --> DB3[(Database<br/>Pipeline Table)]
    
    subgraph Triggers [Scheduler Triggers]
        T1[Cron Trigger<br/>from Schedule Table]
        T2[Interval Trigger<br/>from Schedule Table]
        T3[Manual Trigger<br/>via API]
        T4[Event Trigger<br/>e.g., entity graph updated]
    end
    
    DB1 --> S[Scheduler Agent<br/>Loads from DB]
    DB2 --> S
    DB3 --> S
    DB4[(AgentRegistry<br/>Table)] --> S
    
    T1 --> S
    T2 --> S
    T3 --> S
    T4 --> S
    
    S --> S1[Load Schedule<br/>from Database]
    S1 --> S2[Load JobTemplate<br/>from Database]
    S2 --> S3[Load Agent Metadata<br/>from AgentRegistry]
    
    S3 --> S4{Has Parameter<br/>Expansion?}
    
    S4 -->|Yes - Ticker Expansion| S5[Load Tickers from DB<br/>Based on scope<br/>e.g., 100 tickers]
    S4 -->|Yes - User Expansion| S6[Load Users from DB<br/>Based on filters]
    S4 -->|Yes - Both| S7[Load User-Ticker<br/>Combinations]
    S4 -->|No| S8[Create Single Job]
    
    S5 --> S9[Batch Tickers<br/>batchSize: 10-20<br/>100 tickers → 5-10 batches]
    S9 --> S10[Create Jobs per Ticker<br/>100 tickers = 100 jobs]
    S10 --> S11[Enqueue Jobs in Batches<br/>with staggerDelay]
    
    S6 --> S12[Create Jobs per User]
    S12 --> S11
    
    S7 --> S13[Create Jobs per<br/>User-Ticker Pair]
    S13 --> S11
    
    S8 --> S11
    
    S1 --> S14{Is Pipeline?}
    S14 -->|Yes| S15[Load Pipeline Steps<br/>from Database]
    S15 --> S16[Build Dependency Graph]
    S16 --> S17[Create Jobs for Each Step<br/>with Dependencies]
    S17 --> S11
    
    S11 --> Q[Job Queue - BullMQ]
    
    subgraph Queue [Job Queue - BullMQ]
        Q1[Agent Jobs<br/>Processed by Workers]
    end
    
    Q --> Queue
    
    subgraph Agents [Agents Execute Independently - Multiple Workers]
        direction TB
        AG1[Agent Workers<br/>Process jobs<br/>Read/write to DB<br/>Mark job complete]
    end
    
    Queue --> AG1
    
    AG1 --> DB5[(Database<br/>All agents read/write<br/>independently)]
    
    DB5 --> S18[Scheduler queries DB<br/>for job status]
    S18 --> S19[Update ScheduleExecution<br/>Table]
    S19 --> R[Return orchestration results]
"
/>
