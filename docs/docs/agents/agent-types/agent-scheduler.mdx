---
title: Scheduler Agent
---

## Purpose

Orchestrate and schedule all agents in the system by managing periodic tasks, pipeline workflows, and job dependencies. The scheduler is **completely generic** and **database-driven**—it has no hardcoded agent configurations. Admins create schedules, job templates, and pipelines through the admin interface, and the scheduler dynamically discovers agents and executes jobs based on database configurations.

The scheduler invokes agents via HTTP endpoints based on schedules stored in the database (cron, interval, or on-demand). Agents are **language-agnostic** and can run anywhere (containers, cloud functions, n8n workflows, etc.) as long as they follow the [Agent Specification](./agent-specification). Agents execute asynchronously, reading from and writing to the database. The scheduler does not wait for agent completion but tracks job status through the database.

**Key Responsibilities**:
- Dynamically discover agents from the database (no hardcoded agent list)
- Execute schedules created by admins (stored in `Schedule` table)
- Expand job templates with parameter expansion (e.g., one schedule → many jobs for many tickers)
- Orchestrate multi-agent pipelines with job dependencies (stored in `Pipeline` table)
- Manage different schedule types: cron expressions, intervals, and on-demand triggers
- Invoke agent HTTP endpoints with proper parameters and authentication
- Monitor job status through database queries (`AgentJobExecution` table)
- Handle retries for failed jobs by re-invoking agent endpoints
- Support manual triggers via API or admin interface
- Validate agent responses and handle agent failures gracefully

## Inputs

```typescript
interface SchedulerInput {
  // For scheduled tasks (from database)
  scheduleId?: string; // ID of schedule from Schedule table
  
  // For manual/on-demand triggers
  trigger?: {
    type: 'manual' | 'event-driven',
    jobTemplateId?: string; // Reference to JobTemplate in database
    agentId?: string; // Agent ID (must exist in AgentRegistry)
    params?: Record<string, any>; // Parameters to pass to job (merged with template params)
    expansionParams?: {
      // Parameter expansion configuration
      expandTickers?: {
        scope: 'all' | 'active' | string[]; // Which tickers to expand
        batchSize?: number;
        staggerDelay?: number;
      };
      expandUsers?: {
        userIds?: string[]; // Specific users, or all if empty
        tickerIds?: string[]; // Filter by ticker subscriptions
      };
    };
  },
  
  // For pipeline execution
  pipelineId?: string; // ID of pipeline from Pipeline table
  pipelineParams?: Record<string, any>; // Parameters for pipeline execution
}
```

## Database Schema

The scheduler uses the following database tables (stored in PostgreSQL):

### `AgentRegistry`
Stores metadata about all available agents in the system. Agents register themselves here. See [Agent Specification](./agent-specification) for full registration details.

```typescript
{
  id: string; // Agent ID (e.g., 'query-strategy', 'data-collection')
  name: string; // Human-readable name
  description: string;
  version: string;
  endpoint: {
    type: 'http' | 'webhook' | 'n8n' | 'cloud-function';
    url: string; // Full URL to agent endpoint
    method: 'POST' | 'PUT';
    authentication?: {
      type: 'bearer' | 'api-key' | 'oauth';
      token?: string;
    };
    timeout: number; // Request timeout in milliseconds
    retryConfig?: {
      maxRetries: number;
      backoff: 'exponential' | 'linear' | 'fixed';
      delay: number;
    };
  };
  inputSchema: object; // JSON Schema or Zod schema (JSON representation)
  outputSchema: object; // JSON Schema or Zod schema (JSON representation)
  parameterTypes: {
    // Defines what parameters this agent accepts and how they can be expanded
    ticker?: {
      required: boolean;
      expandable: boolean; // If true, scheduler can expand this to multiple jobs
      scope: 'all' | 'active' | 'custom'; // Default scope for expansion
    };
    userId?: {
      required: boolean;
      expandable: boolean;
    };
    // ... other parameter types
  };
  enabled: boolean;
  healthCheck?: {
    endpoint: string;
    interval: number;
  };
  metadata?: {
    language?: string; // Implementation language
    runtime?: string; // Runtime environment
    deployment?: string; // Deployment type
  };
  lastHeartbeat?: Date; // Last health check timestamp
  createdAt: Date;
  updatedAt: Date;
}
```

### `Schedule`
Stores all schedules created by admins. No hardcoded schedules.

```typescript
{
  id: string;
  name: string;
  description?: string;
  type: 'cron' | 'interval' | 'on-demand';
  cronExpression?: string; // For cron type
  interval?: number; // For interval type (milliseconds)
  timezone?: string; // Default: 'America/New_York'
  enabled: boolean;
  jobTemplateId: string; // References JobTemplate
  expansionConfig?: {
    // How to expand parameters when creating jobs
    expandTickers?: {
      scope: 'all' | 'active' | string[]; // Which tickers to expand
      batchSize?: number; // Number of tickers per batch
      staggerDelay?: number; // Delay between batches (ms)
    };
    expandUsers?: {
      userIds?: string[]; // Specific users, or all if empty
      tickerIds?: string[]; // Filter by ticker subscriptions
    };
  };
  priority?: number; // Job priority (default: 0)
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `JobTemplate`
Defines reusable job templates that can be used by schedules or triggered manually.

```typescript
{
  id: string;
  name: string;
  description?: string;
  agentId: string; // References AgentRegistry
  defaultParams: object; // Default parameters (JSON)
  requiredParams?: string[]; // Parameter names that must be provided
  expansionParams?: string[]; // Parameter names that can be expanded (e.g., ['ticker', 'userId'])
  retryConfig?: {
    maxRetries: number;
    backoff: 'exponential' | 'linear' | 'fixed';
    delay: number;
  };
  timeout?: number; // Job timeout in milliseconds
  priority?: number; // Default job priority
  enabled: boolean;
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `Pipeline`
Defines multi-agent workflows with dependencies.

```typescript
{
  id: string;
  name: string;
  description?: string;
  steps: Array<{
    stepId: string; // Unique ID within pipeline
    jobTemplateId: string; // References JobTemplate
    dependsOn?: string[]; // Step IDs this step depends on
    parallel: boolean; // Can run in parallel with other steps
    condition?: {
      // Optional condition for step execution
      type: 'data-freshness' | 'custom';
      config: object;
    };
    retryConfig?: {
      maxRetries: number;
      backoff: 'exponential' | 'linear' | 'fixed';
      delay: number;
    };
    timeout?: number;
  }>;
  expansionConfig?: {
    // How to expand parameters for pipeline execution
    expandTickers?: {
      scope: 'all' | 'active' | string[];
      batchSize?: number;
      staggerDelay?: number;
    };
    expandUsers?: {
      userIds?: string[];
      tickerIds?: string[];
    };
  };
  enabled: boolean;
  createdAt: Date;
  updatedAt: Date;
  createdBy: string; // Admin user ID
}
```

### `ScheduleExecution`
Tracks schedule execution history.

```typescript
{
  id: string;
  scheduleId: string; // References Schedule
  executionTime: Date;
  status: 'success' | 'partial' | 'failed';
  jobsCreated: number;
  jobsEnqueued: number;
  errors?: Array<{
    message: string;
    timestamp: Date;
  }>;
  metadata?: object; // Additional execution metadata
}
```

## Configurations

This is the minimal configuration for the scheduler agent. It is stored in the `AgentConfig` table, key: `scheduler`.

```typescript
{
  // Global scheduler settings
  scheduler: {
    enabled: true,
    timezone: 'America/New_York', // Default timezone for schedules
    checkInterval: 60000 // How often to check for due schedules (ms)
  },
  
  // HTTP client configuration for agent invocations
  httpClient: {
    timeout: 300000, // Default timeout (ms) - can be overridden per agent
    maxConcurrentRequests: 50, // Max concurrent agent invocations
    retryConfig: {
      maxRetries: 3,
      backoff: { type: 'exponential', delay: 5000 }
    },
    connectionPool: {
      maxSockets: 100,
      keepAlive: true
    }
  },
  
  // Default expansion settings (can be overridden per schedule)
  defaultExpansion: {
    tickerBatchSize: 10,
    tickerStaggerDelay: 1000,
    userBatchSize: 20,
    userStaggerDelay: 500
  },
  
  // Monitoring and notifications
  monitoring: {
    enabled: true,
    notifyOnFailure: true,
    notificationChannels: ['email', 'slack'] // Optional
  }
}
```

**Note**: All agent-specific schedules, job templates, and pipelines are stored in the database tables above, not in the scheduler configuration. The scheduler dynamically loads and executes them.

## Outputs

```typescript
{
  agentId: 'scheduler',
  agentVersion: string,                 // Semantic version (e.g., "1.2.3") of the agent that generated this output
  executionId: string,
  timestamp: Date,
  executionTime: number,
  triggerType: 'cron' | 'interval' | 'manual' | 'event-driven',
  schedulesExecuted?: Array<{
    scheduleId: string,
    scheduleName: string,
    jobTemplateId: string,
    jobTemplateName: string,
    agentId: string,
    jobsCreated: number,
    parametersExpanded?: {
      tickers?: number, // Number of tickers expanded
      users?: number, // Number of users expanded
      totalCombinations?: number // Total expanded parameter sets
    },
    batchesProcessed?: number, // For batched processing
    status: 'success' | 'partial' | 'failed'
  }>,
  jobsEnqueued: Array<{
    jobId: string,
    agentId: string,
    jobTemplateId?: string, // If created from template
    scheduleId?: string, // If part of a scheduled task
    pipelineId?: string, // If part of a pipeline
    pipelineStepId?: string, // If part of a pipeline step
    status: 'pending' | 'running' | 'completed' | 'failed',
    priority: number,
    enqueuedAt: Date,
    dependencies?: string[], // Job IDs this job depends on
    params?: Record<string, any> // Agent-specific parameters (merged from template + trigger)
  }>,
  pipelinesExecuted?: Array<{
    pipelineId: string,
    pipelineName: string,
    status: 'initiated' | 'in-progress' | 'completed' | 'partial',
    parametersExpanded?: {
      tickers?: number,
      users?: number,
      totalCombinations?: number
    },
    stepsExecuted: Array<{
      stepId: string,
      jobTemplateId: string,
      jobsCreated: number,
      status: 'pending' | 'running' | 'completed' | 'failed'
    }>,
    conditionalSteps?: {
      dataFreshnessChecks?: number,
      dataCollectionJobsTriggered?: number,
      freshDataCount?: number,
      staleDataCount?: number
    },
    metadata: {
      totalJobsCreated: number,
      totalSteps: number,
      completedSteps: number
    }
  }>,
  errors?: Array<{
    scheduleId?: string,
    jobTemplateId?: string,
    pipelineId?: string,
    agentId?: string,
    error: string,
    timestamp: Date
  }>
}
```

**Note**: The scheduler returns orchestration results immediately after invoking agent endpoints. Actual job and pipeline status is tracked in the database (`AgentJobExecution` and `ScheduleExecution` tables) and can be queried separately. Agents update their job status in the database as they execute independently.

## Process

1. **Initialize**: 
   - Load scheduler config from database (`AgentConfig` table, key: `scheduler`)
   - Load all enabled schedules from `Schedule` table
   - Load all enabled job templates from `JobTemplate` table
   - Load all enabled pipelines from `Pipeline` table
   - Load agent registry from `AgentRegistry` table (discover available agents)
   - Validate agent endpoints are accessible (health checks)
   - Initialize cron scheduler for all cron-based schedules
   - Initialize interval timers for all interval-based schedules
   - Initialize job dependency tracking (for pipelines)
   - Initialize ticker/identifier cache (optional, for performance when processing many tickers)
   - Initialize HTTP client pool for agent invocations

2. **Agent Discovery**:

   - Query `AgentRegistry` table for all enabled agents
   - Validate that agents referenced in schedules, job templates, and pipelines exist in registry
   - Verify agent endpoints are accessible (health checks)
   - Load agent input schemas to validate job parameters
   - Cache agent metadata (endpoints, authentication, timeouts) for performance
   - Monitor agent heartbeats and mark agents as unavailable if heartbeat is stale

3. **Schedule Execution** (triggered by cron, interval, manual, or event):

   The scheduler handles different types of schedules from the database:

   **3a. Cron-based Schedules**:
   - Monitor all cron expressions from `Schedule` table where `type = 'cron'` and `enabled = true`
   - When cron expression matches current time, execute the corresponding schedule
   - Load the associated `JobTemplate` from the schedule's `jobTemplateId`
   - Execute job creation with parameter expansion (see section 4)

   **3b. Interval-based Schedules**:
   - Monitor interval timers for schedules where `type = 'interval'` and `enabled = true`
   - When interval elapses, execute the corresponding schedule
   - Load the associated `JobTemplate` and execute job creation

   **3c. On-demand Triggers**:
   - Handle manual triggers via API or admin interface
   - Handle event-driven triggers (e.g., entity graph updated → trigger query generation)
   - Execute immediately when triggered
   - Can reference a `JobTemplate` directly or provide ad-hoc parameters

4. **Parameter Expansion** (for agents that require expansion):

   The scheduler expands parameters based on `expansionConfig` in schedules or `expansionParams` in triggers:

   **4a. Ticker Expansion**:
   - If `expandTickers` is configured:
     - Load tickers/identifiers from database based on `scope`:
       - `'all'`: All tickers in the `Ticker` table where `enabled = true`
       - `'active'`: Only tickers with active user subscriptions (from `UserTicker` table)
       - `string[]`: Specific ticker identifiers provided in configuration
     - Filter by enabled/active status
     - Group tickers into batches based on `batchSize` configuration
     - Apply stagger delays between batches to avoid rate limits
   - For each ticker, create a separate job with `ticker` parameter set

   **4b. User Expansion**:
   - If `expandUsers` is configured:
     - Load users from database based on filters:
       - If `userIds` provided: Only those specific users
       - If `tickerIds` provided: Users subscribed to those tickers
       - Otherwise: All active users
     - Group users into batches if needed
   - For each user, create a separate job with `userId` parameter set

   **4c. Combined Expansion** (e.g., user-ticker pairs):
   - Expand both users and tickers
   - Create jobs for each user-ticker combination
   - Example: 10 users × 5 tickers = 50 jobs

5. **Job Creation** (from JobTemplate):

   For each schedule execution or manual trigger:

   **5a. Load JobTemplate**:
   - Load the `JobTemplate` referenced by the schedule or trigger
   - Validate that the template's `agentId` exists in `AgentRegistry`
   - Merge default parameters from template with trigger parameters

   **5b. Parameter Expansion**:
   - Check if any parameters in `expansionParams` need expansion
   - If ticker expansion needed: Expand to multiple tickers (see 4a)
   - If user expansion needed: Expand to multiple users (see 4b)
   - If both: Expand to user-ticker combinations (see 4c)

   **5c. Invoke Agents**:
   - For each expanded parameter set (or single set if no expansion):
     - Generate unique `jobId` and `executionId`
     - Create execution record in `AgentJobExecution` table with `status: 'pending'`
     - Build execution request with merged parameters (template defaults + trigger params + expanded params)
     - Validate parameters against agent's `inputSchema`
     - Invoke agent HTTP endpoint (POST to `AgentRegistry.endpoint.url`)
       - Include authentication headers if configured
       - Set timeout from agent endpoint config or template
       - Include `X-Job-Id` and `X-Execution-Id` headers
     - Handle agent response:
       - If `status: 'accepted'`: Job is queued for execution by agent
       - If `status: 'rejected'`: Mark execution as failed, log error
     - Apply stagger delays between batches (if configured)
   - Track job creation in `ScheduleExecution` table

6. **Pipeline Orchestration**:

   For pipeline execution (from `Pipeline` table):

   **6a. Load Pipeline**:
   - Load pipeline definition from `Pipeline` table
   - Validate all referenced `JobTemplate` IDs exist
   - Validate all referenced agents exist in `AgentRegistry`

   **6b. Parameter Expansion** (if configured):
   - Apply `expansionConfig` from pipeline (same as schedule expansion)
   - Expand tickers, users, or both as needed

   **6c. Build Dependency Graph**:
   - Parse pipeline `steps` to build dependency graph
   - Identify steps that can run in parallel (`parallel: true`)
   - Identify steps that must run sequentially (via `dependsOn`)

   **6d. Execute Pipeline Steps**:
   - For each expanded parameter set (e.g., each user-ticker pair):
     - Create execution records for all pipeline steps in `AgentJobExecution` table
     - Build dependency graph based on `dependsOn` relationships
     - For steps without dependencies: Invoke agent endpoints immediately
     - For steps with dependencies: Wait for dependency jobs to complete (check `AgentJobExecution.status`)
     - Handle conditional steps (e.g., data freshness checks)
     - Invoke agent endpoints with proper sequencing
   - Track pipeline execution in database

   **6e. Conditional Steps**:
   - If step has `condition.type = 'data-freshness'`:
     - Check timestamp of latest collected data in database
     - If data is stale, invoke Data Collection agent endpoint with high priority first
     - Wait for Data Collection job to complete (poll `AgentJobExecution.status`)
     - Then invoke the dependent pipeline step
   - Custom conditions can be implemented via plugins

7. **Agent Execution**:

   - Scheduler invokes agent HTTP endpoints asynchronously
   - Agents accept jobs immediately and return `status: 'accepted'`
   - Agents execute work asynchronously and update `AgentJobExecution` table
   - Agents do not receive data from other agents
   - Agents read from and write to database independently
   - No direct agent-to-agent communication
   - All coordination through database state
   - Scheduler monitors agent execution by querying `AgentJobExecution` table

8. **Job Dependencies & Status Tracking**:

   - Track job status in `AgentJobExecution` table (pending, running, completed, failed)
   - For pipeline dependencies: Poll `AgentJobExecution.status` to wait for upstream jobs
   - Monitor job progress by querying `AgentJobExecution` table periodically
   - Handle failed jobs with retry logic:
     - Check `AgentJobExecution.error.retryable` flag
     - If retryable and retry count < max: Re-invoke agent endpoint after backoff delay
     - If not retryable: Mark as permanently failed, notify admins
   - Use exponential backoff from template/schedule config for retries

9. **Parallelization**:

   - **Expanded Parameter Jobs** (e.g., per-ticker, per-user):
     - Multiple expanded items processed in parallel (controlled by `batchSize` in expansion config)
     - Scheduler invokes multiple agent endpoints concurrently (up to `batchSize`)
     - Example: 100 tickers with `batchSize: 20` = 5 batches, each batch invokes 20 agent endpoints in parallel
     - Stagger delays between batches prevent rate limiting and resource exhaustion
     - Agents execute work asynchronously after accepting jobs
   
   - **Single Parameter Jobs** (no expansion):
     - Single job processes all data in aggregate
     - Can run concurrently with other agent jobs
   
   - **Pipeline Steps**:
     - Steps with `parallel: true` can run concurrently
     - Steps with `dependsOn` run sequentially after dependencies complete
     - Multiple parameter expansions (e.g., user-ticker pairs) can be processed in parallel
     - Example: Newsletter pipeline with 10 users × 5 tickers = 50 parallel pipeline executions

10. **Scalability Considerations**:

   - **Large Parameter Sets**: When processing hundreds of tickers/users:
     - Batch processing prevents overwhelming agent endpoints
     - Stagger delays prevent API rate limits and agent overload
     - Multiple agent invocations happen concurrently (up to `batchSize`)
     - Failed jobs for one parameter set don't block others
     - Partial failures are handled gracefully (continue with remaining items)
     - Agents can scale independently to handle their own load
   
   - **Dynamic Management**:
     - New tickers/users are automatically included in next scheduled run (if scope includes them)
     - Disabled tickers/users are excluded from processing
     - Priority can be set per schedule/template to influence job priority
   
   - **Resource Management**:
     - `batchSize` in expansion config controls concurrent job execution
     - Queue concurrency settings limit total parallel workers
     - Memory and CPU usage scales with batch size and concurrency

11. **Error Handling**:

   - **Agent Invocation Errors** (network, timeout, agent unavailable):
     - Retry with exponential backoff (configured in `JobTemplate` or `Schedule`)
     - Mark agent as unavailable if health check fails
     - Log errors to database for monitoring (`ScheduleExecution` table)
   
   - **Agent Execution Errors** (agent rejects job or fails during execution):
     - Agent updates `AgentJobExecution.status: 'failed'` with error details
     - Scheduler checks `error.retryable` flag
     - If retryable: Re-invoke agent endpoint after backoff delay
     - If not retryable: Mark as permanently failed, notify admins
   
   - **Pipeline Dependencies**:
     - Job dependencies can timeout if upstream jobs fail repeatedly
     - Pipeline continues with partial results if some jobs fail (non-blocking)
   
   - **General**:
     - Notify on critical failures (via configured notification channels)
     - Failed jobs for one schedule/template don't block others
     - Agents can implement their own retry logic internally

12. **Metrics Collection**:

   - Agents write metrics to database upon completion
   - Scheduler queries database for job and pipeline status
   - Track schedule execution history in `ScheduleExecution` table
   - Learning Agent reads metrics from database independently (runs on its own schedule)

13. **Return**: Orchestration results (job IDs, status, metadata for all scheduled agents, including expanded parameter counts)

## Admin Interface Usage

Admins can create and manage schedules, job templates, and pipelines through the admin interface. The scheduler dynamically loads and executes these configurations without any code changes.

### Creating a Schedule

1. **Create a JobTemplate** (if it doesn't exist):
   - Navigate to `/admin/scheduler/job-templates`
   - Click "Create Job Template"
   - Select an agent from `AgentRegistry`
   - Define default parameters
   - Configure retry settings and timeout
   - Save the template

2. **Create a Schedule**:
   - Navigate to `/admin/scheduler/schedules`
   - Click "Create Schedule"
   - Select a `JobTemplate`
   - Choose schedule type (cron, interval, or on-demand)
   - Configure schedule timing (cron expression or interval)
   - Configure parameter expansion (if needed):
     - Enable ticker expansion: Select scope (all, active, or specific tickers)
     - Enable user expansion: Select users or filter by ticker subscriptions
     - Set batch size and stagger delay
   - Save the schedule

### Creating a Pipeline

1. **Ensure JobTemplates exist** for all pipeline steps

2. **Create a Pipeline**:
   - Navigate to `/admin/scheduler/pipelines`
   - Click "Create Pipeline"
   - Add pipeline steps:
     - Select a `JobTemplate` for each step
     - Define step dependencies (`dependsOn`)
     - Configure parallel execution (`parallel: true/false`)
     - Add conditional logic (e.g., data freshness checks)
   - Configure parameter expansion (if needed)
   - Save the pipeline

3. **Create a Schedule for the Pipeline**:
   - Create a schedule that references the pipeline (instead of a single job template)
   - The scheduler will execute the pipeline when the schedule triggers

### Example: Query Strategy Agent Schedule

**JobTemplate**:
- `id`: "query-strategy-entity-graph"
- `agentId`: "query-strategy"
- `defaultParams`: `{ "refreshEntityGraph": true }`
- `expansionParams`: `["ticker"]`

**Schedule**:
- `name`: "Weekly Entity Graph Refresh"
- `type`: "cron"
- `cronExpression`: "0 2 * * 0" (Sunday 2 AM)
- `jobTemplateId`: "query-strategy-entity-graph"
- `expansionConfig.expandTickers`: `{ scope: "all", batchSize: 10, staggerDelay: 1000 }`

**Result**: Every Sunday at 2 AM, the scheduler expands to all tickers and creates 100 jobs (one per ticker) that process in batches of 10.

## Sequence Diagram

<Mermaid
  chart="
flowchart TD
    subgraph Admin [Admin Interface]
        A1[Admin Creates<br/>JobTemplate]
        A2[Admin Creates<br/>Schedule]
        A3[Admin Creates<br/>Pipeline]
    end
    
    A1 --> DB1[(Database<br/>JobTemplate Table)]
    A2 --> DB2[(Database<br/>Schedule Table)]
    A3 --> DB3[(Database<br/>Pipeline Table)]
    
    subgraph Triggers [Scheduler Triggers]
        T1[Cron Trigger<br/>from Schedule Table]
        T2[Interval Trigger<br/>from Schedule Table]
        T3[Manual Trigger<br/>via API]
        T4[Event Trigger<br/>e.g., entity graph updated]
    end
    
    DB1 --> S[Scheduler Agent<br/>Loads from DB]
    DB2 --> S
    DB3 --> S
    DB4[(AgentRegistry<br/>Table)] --> S
    
    T1 --> S
    T2 --> S
    T3 --> S
    T4 --> S
    
    S --> S1[Load Schedule<br/>from Database]
    S1 --> S2[Load JobTemplate<br/>from Database]
    S2 --> S3[Load Agent Metadata<br/>from AgentRegistry]
    
    S3 --> S4{Has Parameter<br/>Expansion?}
    
    S4 -->|Yes - Ticker Expansion| S5[Load Tickers from DB<br/>Based on scope<br/>e.g., 100 tickers]
    S4 -->|Yes - User Expansion| S6[Load Users from DB<br/>Based on filters]
    S4 -->|Yes - Both| S7[Load User-Ticker<br/>Combinations]
    S4 -->|No| S8[Create Single Job]
    
    S5 --> S9[Batch Tickers<br/>batchSize: 10-20<br/>100 tickers → 5-10 batches]
    S9 --> S10[Create Execution Records<br/>100 tickers = 100 executions]
    S10 --> S11[Invoke Agent HTTP Endpoints<br/>in Batches with staggerDelay]
    
    S6 --> S12[Invoke Agent per User]
    S12 --> S11
    
    S7 --> S13[Invoke Agent per<br/>User-Ticker Pair]
    S13 --> S11
    
    S8 --> S11
    
    S1 --> S14{Is Pipeline?}
    S14 -->|Yes| S15[Load Pipeline Steps<br/>from Database]
    S15 --> S16[Build Dependency Graph]
    S16 --> S17[Invoke Agents for Each Step<br/>with Dependencies]
    S17 --> S11
    
    S11 --> AG1[Agent HTTP Endpoints<br/>POST /execute]
    
    subgraph Agents [Agents Execute Independently - Any Language/Platform]
        direction TB
        AG2[Agent Accepts Job<br/>Returns status: accepted]
        AG3[Agent Executes Work<br/>Asynchronously]
        AG4[Agent Updates DB<br/>AgentJobExecution table]
        AG5[Agent Writes Results<br/>to Database]
    end
    
    AG1 --> AG2
    AG2 --> AG3
    AG3 --> AG4
    AG4 --> AG5
    
    AG5 --> DB5[(Database<br/>All agents read/write<br/>independently)]
    
    DB5 --> S18[Scheduler queries DB<br/>AgentJobExecution table<br/>for job status]
    S18 --> S19[Update ScheduleExecution<br/>Table]
    S19 --> R[Return orchestration results]
"
/>
