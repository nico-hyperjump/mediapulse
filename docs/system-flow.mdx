---
title: System Flow
---

## System Architecture Overview

The system uses a **modular, event-driven architecture** where agents run independently on their own schedules, communicating through a shared data store and event queue. This design provides:

- **Scalability**: Agents can be scaled independently based on load
- **Reliability**: Failures in one agent don't block others
- **Flexibility**: Each agent runs on its optimal schedule
- **Modularity**: Agents are loosely coupled and can be updated independently
- **Extensibility**: Plugin-based architecture allows dynamic addition/removal of capabilities (e.g., analysis types) without code changes

## Agent Schedules & Independence

<Mermaid
  chart="
flowchart TB
    subgraph Continuous [Continuous Data Pipeline]
        QS[Query Strategy Agent<br/>• Scheduled via JobTemplate<br/>• Admin-configurable schedules<br/>• Query Generation: On-demand]
        DC[Data Collection Agent<br/>• Scheduled via JobTemplate<br/>• Admin-configurable intervals<br/>• Independent of Query Strategy<br/>• Uses latest queries from DB]
    end
    
    subgraph Newsletter [Newsletter Generation Pipeline]
        Orchestrator[Scheduler/Orchestrator<br/>• Not an agent<br/>• Database-driven schedules<br/>• Admin-configurable<br/>• Executes pipelines from DB<br/>• Distributes jobs across instances]
        Analysis[Analysis Agent<br/>• Triggered by Orchestrator<br/>• Reads collected data from DB<br/>• Works independently<br/>• Multiple instances can run in parallel]
        Content[Content Generation Agent<br/>• Triggered by Orchestrator<br/>• Reads analysis from DB<br/>• Works independently<br/>• Multiple instances can run in parallel<br/>• Processes one user-ticker pair per job]
        QA[Quality Assurance Agent<br/>• Triggered by Orchestrator<br/>• Reads newsletter from DB<br/>• Works independently<br/>• Multiple instances can run in parallel]
        Delivery[Delivery Agent<br/>• Triggered by Orchestrator<br/>• Reads approved newsletter from DB<br/>• Works independently<br/>• Multiple instances can run in parallel]
    end
    
    subgraph Background [Background Agents]
        Learning[Learning Agent<br/>• Scheduled via JobTemplate<br/>• Admin-configurable<br/>• Analyzes all metrics]
    end
    
    QS -->|Writes entity graph<br/>& queries to DB| DB[(Shared Database)]
    DC -->|Writes collected<br/>data to DB| DB
    DB -->|Reads queries| DC
    DB -->|Reads collected data| Analysis
    Orchestrator -->|Triggers| Analysis
    Analysis -->|Writes analysis<br/>results to DB| DB
    DB -->|Reads analysis| Content
    Orchestrator -->|Triggers| Content
    Content -->|Writes newsletter<br/>to DB| DB
    DB -->|Reads newsletter| QA
    Orchestrator -->|Triggers| QA
    QA -->|Writes approval<br/>status to DB| DB
    DB -->|Reads approved<br/>newsletter| Delivery
    Orchestrator -->|Triggers| Delivery
    Delivery -->|Writes metrics| DB
    Learning -->|Reads metrics<br/>Updates configs| DB
"
/>

## Independent Agent Flows

### Query Strategy Agent

Triggers:

- Schedule
- Manual trigger

<Mermaid
  chart="
flowchart TD
    A[Query Strategy Agent<br/>Triggered by Schedule<br/>or Manual trigger] --> B{Entity Graph<br/>Refresh Needed?}
    B -->|Yes<br/>Weekly| C[Entity Discovery<br/>• Query regulatory APIs<br/>• Scrape company websites<br/>• Extract relationships]
    B -->|No| D[Query Optimization<br/>• Analyze query performance<br/>• Generate optimized queries]
    C --> E[Update Entity Graph<br/>in Database]
    D --> F[Generate New Queries<br/>Store in Database]
    E --> G[Query Generation<br/>• Use entity graph<br/>• Generate search queries<br/>• Store in Database]
    F --> G
    G --> H[Database<br/>Queries available for<br/>Data Collection Agent]
"
/>

**Schedule**:

- Schedules are created by admins via the admin interface
- Each schedule references a `JobTemplate` that defines the agent and parameters
- Parameter expansion can be configured (e.g., expand to all tickers)
- Examples:
  - Entity Graph Refresh: Weekly schedule with ticker expansion
  - Query Optimization: Daily schedule with ticker expansion
  - Query Generation: On-demand or triggered by events

### Data Collection Agent

Triggers:

- Schedule
- Manual trigger

<Mermaid
  chart="
flowchart TD
    A[Data Collection Agent<br/>Triggered by Schedule<br/>or Manual trigger] --> B[Load Latest Queries<br/>from Database<br/>Generated by Query Strategy Agent]
    B --> C[Check Last Collection Time<br/>for each ticker]
    C --> D{Collection<br/>Needed?}
    D -->|Yes| E[Query Search Sources<br/>• Serper.dev<br/>• Google Search API<br/>• Other admin-configured sources<br/>Parallel across sources]
    D -->|No<br/>Recent data exists| F[Skip Collection]
    E --> G[Extract URLs from<br/>Search Results]
    G --> H[Fetch Web Pages<br/>Parallel fetching]
    H --> I[Process & Deduplicate<br/>• Filter low-quality content<br/>• Remove duplicates<br/>• Extract metadata]
    I --> J[Store in Database<br/>with timestamp]
    J --> K[Database<br/>Fresh data available for<br/>Analysis Agent]
"
/>

**Schedule**:

- Schedules are created by admins via the admin interface
- Each schedule references a `JobTemplate` for the Data Collection agent
- Parameter expansion can be configured to process multiple tickers
- Runs independently - doesn't wait for Query Strategy
- Example: Interval-based schedule (e.g., every 4 hours) with ticker expansion

### Newsletter Generation Pipeline

Triggers:

- Schedule
- Manual trigger

<Mermaid
  chart="
flowchart TD
    A[Newsletter Generation Pipeline<br/>Triggered by Schedule<br/>or Manual trigger] --> B[For each user-ticker<br/>subscription]
    B --> C[Check Data Freshness<br/>in Database]
    C --> D{Data Fresh<br/>enough?}
    D -->|Yes<br/>< 4 hours old| E[Analysis Agent<br/>• Loads enabled analysis plugins<br/>• Reads collected data<br/>• Executes registered analysis types<br/>• Dynamic parallel execution]
    D -->|No<br/>Stale data| F[Enqueue Data Collection Job<br/>High priority]
    F --> F1{Wait for<br/>collection?}
    F1 -->|Yes| F2[Wait for collection<br/>to complete]
    F1 -->|No - default| E[Use existing data<br/>Continue pipeline]
    F2 --> E
    E --> G[Store Analysis Results<br/>in Database<br/>Generic structure with all types]
    G --> H[Content Generation Agent<br/>• Triggered by Orchestrator<br/>• Reads analysis from DB<br/>• Reads user prefs from DB<br/>• Generates newsletter<br/>• Writes newsletter to DB<br/>• Multiple instances can run in parallel<br/>• Each job processes one user-ticker pair]
    H --> I[Quality Assurance Agent<br/>• Triggered by Orchestrator<br/>• Reads newsletter from DB<br/>• Validates content<br/>• Checks compliance<br/>• Writes approval status to DB<br/>• Multiple instances can run in parallel]
    I --> J{Approved?}
    J -->|Yes| K[Delivery Agent<br/>• Triggered by Orchestrator<br/>• Reads approved newsletter from DB<br/>• Sends email<br/>• Updates dashboard<br/>• Writes metrics to DB<br/>• Multiple instances can run in parallel]
    J -->|No| H
    K --> L[Store Delivery Metrics<br/>in Database]
"
/>

**Schedule**:

- Pipeline is defined in the `Pipeline` table by admins
- Schedule references the pipeline (stored in `Schedule` table)
- Admin-configurable timing (cron, interval, or on-demand)
- Parameter expansion can be configured (e.g., expand to all user-ticker subscriptions)
- User-defined schedules can be created per user-ticker subscription

## Event-Driven Communication

<Mermaid
  chart="
sequenceDiagram
    participant QS as Query Strategy Agent
    participant DB as Database
    participant DC as Data Collection Agent
    participant Orchestrator as Scheduler/Orchestrator
    participant Analysis as Analysis Agent
    participant Content as Content Gen Agent
    participant QA as QA Agent
    participant Delivery as Delivery Agent
    participant Learning as Learning Agent

    Note over QS: Weekly: Entity Graph Refresh
    QS->>DB: Update entity graph
    QS->>DB: Store generated queries

    Note over DC: Every 2-4 hours: Data Collection
    DC->>DB: Read latest queries (from Query Strategy Agent)
    DC->>DC: Query search sources (e.g., Serper.dev) with queries
    DC->>DC: Fetch web pages from search results
    DC->>DC: Process, deduplicate, and filter content
    DC->>DB: Store collected data (timestamped)

    Note over Orchestrator: Scheduled: Newsletter Generation Pipeline
    Orchestrator->>DB: Check data freshness
    alt Data stale
        Orchestrator->>DC: Trigger immediate collection
        DC->>DB: Store fresh data
    end

    Orchestrator->>Analysis: Trigger analysis (independent execution)
    Analysis->>DB: Read collected data
    Analysis->>Analysis: Perform analysis
    Analysis->>DB: Store analysis results

    Orchestrator->>Content: Trigger content generation (independent execution)
    Content->>DB: Read analysis results
    Content->>DB: Read user prefs
    Content->>Content: Generate newsletter
    Content->>DB: Store newsletter draft

    Orchestrator->>QA: Trigger QA (independent execution)
    QA->>DB: Read newsletter draft
    QA->>QA: Validate content
    QA->>DB: Store approval status
    alt Approved
        Orchestrator->>Delivery: Trigger delivery (independent execution)
        Delivery->>DB: Read approved newsletter
        Delivery->>Delivery: Send email
        Delivery->>DB: Store delivery metrics
        Delivery->>User: Newsletter with feedback buttons
        User->>DB: Submit section feedback (Like/Dislike, Useful/Irrelevant)
    else Not Approved
        QA->>DB: Write requiresRevision: true
        Note over Orchestrator: Orchestrator or retry mechanism<br/>can enqueue Content Gen retry job<br/>based on QA results
    end

    Note over Learning: Daily midnight: Learning
    Learning->>DB: Read all metrics + section feedback
    Learning->>Learning: Analyze patterns
    Learning->>DB: Update agent configs
    Note over QS: Query Strategy reads updated configs from DB on next run

"
/>

## Data Flow & Storage

<Mermaid
  chart="
flowchart LR
    subgraph Agents [Agents]
        QS[Query Strategy]
        DC[Data Collection]
        Analysis[Analysis]
        Content[Content Gen]
        Delivery[Delivery]
        Learning[Learning]
    end
    
    subgraph Storage [Shared Storage Layer]
        DB[(PostgreSQL<br/>• Entity Graph<br/>• Queries<br/>• Collected Data<br/>• Analysis Results<br/>• Newsletters<br/>• Metrics<br/>• Section Feedback)]
        Cache[(Redis Cache<br/>• Query results<br/>• Rate limits<br/>• Job queue)]
        Queue[(Job Queue<br/>BullMQ<br/>• Agent jobs<br/>• Retries<br/>• Priorities)]
    end
    
    QS -->|Write| DB
    QS -->|Read| DB
    DC -->|Read queries| DB
    DC -->|Write data| DB
    Analysis -->|Read data| DB
    Analysis -->|Write results| DB
    Content -->|Read analysis| DB
    Content -->|Write newsletter| DB
    Delivery -->|Write metrics| DB
    User -->|Submit feedback| DB
    Learning -->|Read metrics + feedback| DB
    Learning -->|Update configs| DB
    
    Agents -->|Jobs| Queue
    Agents -->|Cache| Cache
"
/>

**Note**: All agent outputs written to the database include an `agentVersion` field that specifies the semantic version (e.g., "1.2.3") of the agent that generated the output. This enables traceability, debugging, and supports the agent versioning and experimentation system. Agents read their active version from the `AgentVersionDeployment` table during initialization and include it in all outputs.

## Key Design Principles

### 1. **Loose Coupling**

- Agents don't directly call each other
- Agents don't receive data from other agents
- All communication through shared database
- Agents read from and write to the database independently
- Agents can be updated/deployed independently

### 2. **Independent Scheduling**

- The **Scheduler/Orchestrator** manages all schedules (it is not an agent)
- Each agent has its own optimal schedule (admin-configurable)
- Schedules are stored in the database (`Schedule` table)
- Job templates define agent parameters and expansion rules
- Query Strategy: Admin-configurable schedules (e.g., weekly entity graph, daily optimization)
- Data Collection: Admin-configurable intervals (e.g., every 1-4 hours)
- Newsletter Generation: Admin-configurable pipeline schedules
- Learning: Admin-configurable schedule (e.g., daily at midnight)

### 3. **Data Freshness Management**

- Data Collection Agent maintains fresh data in database
- Newsletter pipeline checks data freshness before analysis
- Can trigger immediate collection if data is stale
- Timestamps on all data for freshness tracking

### 4. **Scalability**

- **Agent Instances**: Multiple instances of the same agent version can run in parallel for horizontal scaling
- **Job Distribution**: Orchestrator distributes jobs across available agent instances using load balancing
- **Job Division**: Large jobs (e.g., 100 keywords) can be divided into smaller sub-jobs and distributed across instances
- **Instance Management**: Each agent instance registers itself and reports capacity/load for intelligent job distribution
- Database connection pooling
- Caching layer for frequently accessed data

### 5. **Reliability**

- Failed jobs are retried with exponential backoff
- Agents continue running even if others fail
- Data is never lost (persisted before processing)
- Circuit breakers for external API calls

### 6. **Modularity**

- Each agent is a separate service/worker
- Can be deployed independently
- Configuration stored in database (hot-reloadable)
- Versioned agent deployments
- **Plugin-based architecture**: Analysis types are plugins registered in the database, allowing addition/removal without code changes
- Dynamic component loading: Agents discover and load components (analysis types, sections) at runtime

## Error Handling & Resilience

### Per-Agent Error Handling

- **Query Strategy**: Retries entity discovery, falls back to cached entity graph, logs errors to database
- **Data Collection**: Continues with other sources if one fails, retries with exponential backoff, marks failed sources for health monitoring
- **Analysis**: Returns partial results if some analysis types fail, logs errors per plugin, continues with successful analyses
- **Content Generation**: Retries with different prompts on failure (up to max retries), falls back to simpler templates if needed
- **QA**: Flags for manual review if repeated failures, writes detailed error information to database
- **Delivery**: Retries with exponential backoff, dead-letter queue for permanently failed deliveries, tracks bounce rates
- **Learning**: Continues analysis even if some metrics are unavailable, logs warnings for incomplete data

### Job Queue Error Handling

- **BullMQ Retry Logic**: All jobs have configurable retry attempts with exponential backoff
- **Failed Jobs**: Jobs that exceed max retries are moved to dead-letter queue for manual review
- **Job Dependencies**: Failed jobs don't block dependent jobs indefinitely; dependencies can timeout
- **Error Logging**: All errors are logged to database with context for debugging

### System-Level Resilience

- **Database Failures**: Agents queue jobs for retry when DB is available, use connection pooling with retries
- **Queue Failures**: Jobs persisted to database as backup, Redis replication for high availability
- **Agent Failures**: Other agents continue operating independently, failed agent jobs are retried automatically
- **Data Staleness**: Newsletter pipeline triggers fresh collection if needed, can proceed with existing data if collection fails
- **Partial Failures**: System continues operating with partial data (e.g., some analysis types fail but others succeed)
- **Circuit Breakers**: External API calls use circuit breakers to prevent cascade failures
- **Health Monitoring**: All agents report health status, failed agents are automatically restarted

## Configuration & Monitoring

### Agent Configuration

- All configs stored in `AgentConfig` table
- Hot-reloadable without restart
- Per-ticker, per-user, and system-wide configs
- Versioned for rollback capability

### Monitoring & Observability

- Each agent emits metrics (execution time, success rate, data quality)
- Metrics stored in `AgentMetrics` table
- Learning Agent analyzes metrics and optimizes configs
- Dashboard shows agent health, data freshness, pipeline status

## Benefits of This Architecture

1. **Scalability**: Scale Data Collection independently from Newsletter Generation
2. **Reliability**: Data Collection failures don't block Newsletter Generation (uses cached data)
3. **Efficiency**: Data Collection runs frequently to keep data fresh, Newsletter uses latest data
4. **Flexibility**: Easy to add new agents or modify schedules
5. **Performance**: Parallel execution where possible, caching for speed
6. **Maintainability**: Clear separation of concerns, easy to debug and update
