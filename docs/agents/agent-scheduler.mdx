---
title: Scheduler Agent
---

## Purpose

Orchestrate the newsletter generation pipeline by enqueuing jobs for independent agent execution. The scheduler checks data freshness, creates jobs for each stage of the pipeline, and enqueues them to the job queue with proper dependencies. Agents execute independently as BullMQ workers, reading from and writing to the database. The scheduler does not wait for agent completion but tracks pipeline status through the database.

**Key Responsibilities**:
- Schedule periodic newsletter generation (daily, weekly, or user-defined)
- Check data freshness and trigger collection if needed
- Create and enqueue jobs for each pipeline stage (analysis, content generation, QA, delivery)
- Manage job dependencies to ensure proper execution order
- Monitor pipeline status through database queries
- Handle retries for failed jobs

## Inputs

- `schedule: ScheduleConfig` - Schedule configuration (daily, weekly, custom)
- `userTickers: Array<{ userId: string, tickers: string[], preferences: UserPreferences }>` - User subscriptions
- `trigger: 'scheduled' | 'manual' | 'event-driven'` - Execution trigger

## Configurations

This is the configuration for the scheduler agent. It is stored in the `AgentConfig` table, key: `scheduler`.

```typescript
{
  scheduler: {
    type: 'cron' | 'interval',
    cronExpression: '0 6 * * *', // Daily at 6 AM
    timezone: 'America/New_York',
    enabled: true
  },
  pipeline: {
    agents: [
      'analysis',
      'content-generation',
      'quality-assurance',
      'delivery'
    ],
    dataFreshness: {
      thresholdHours: 4, // Trigger data collection if data is older than this (default: 4 hours)
      triggerCollectionOnStale: true, // Whether to trigger collection if data is stale
      waitForCollection: false // Whether to wait for collection to complete before proceeding (default: false - uses existing data if collection is in progress)
    },
    parallelism: {
      analysis: true, // Can run for multiple tickers in parallel
      contentGeneration: false, // Sequential per user (personalization)
      qualityAssurance: false,
      delivery: true
    },
    retry: {
      maxRetries: 3,
      backoff: 'exponential',
      delay: 5000
    },
    timeout: {
      analysis: 180000, // 3 min
      contentGeneration: 240000, // 4 min
      qualityAssurance: 120000, // 2 min
      delivery: 300000 // 5 min
    }
  },
  queue: {
    provider: 'bullmq',
    redisUrl: string,
    concurrency: 5,
    defaultJobOptions: {
      attempts: 3,
      backoff: { type: 'exponential', delay: 5000 }
    }
  }
}
```

## Outputs

```typescript
{
  agentId: 'scheduler',
  executionId: string,
  timestamp: Date,
  executionTime: number,
  orchestration: {
    status: 'initiated' | 'in-progress' | 'completed' | 'partial',
    jobsCreated: number,
    jobsEnqueued: Array<{
      jobId: string,
      agent: string,
      stage: string,
      status: 'pending' | 'running' | 'completed' | 'failed',
      priority: number,
      enqueuedAt: Date,
      dependencies?: string[] // Job IDs this job depends on
    }>
  },
  dataFreshness: {
    tickersChecked: number,
    freshDataCount: number,
    staleDataCount: number,
    dataCollectionJobsTriggered: number
  },
  metadata: {
    tickersProcessed: number,
    usersProcessed: number,
    totalJobsCreated: number
  }
}
```

**Note**: The scheduler returns orchestration results immediately after enqueuing jobs. Actual pipeline status is tracked in the database and can be queried separately. Agents update their job status in the database as they execute independently.

## Process

1. **Initialize**: 
   - Load schedule config from database (`AgentConfig` table, key: `scheduler`)
   - Connect to job queue (BullMQ with Redis)
   - Load user-ticker subscriptions from database
   - Initialize job dependency tracking

2. **Data Freshness Check** (for each user-ticker subscription):

   - Check timestamp of latest collected data in database
   - If data is stale (older than `thresholdHours`), enqueue a high-priority Data Collection Agent job
   - If `waitForCollection` is `true`, wait for collection to complete before proceeding (blocks pipeline)
   - If `waitForCollection` is `false` (default), proceed with existing data even if stale (non-blocking)
   - Note: Data Collection Agent also runs independently every 1-4 hours, so this is only needed if data is stale when newsletter generation is triggered

3. **Job Creation** (for each user-ticker subscription):

   - Create independent jobs for each agent in the pipeline
   - Set job priority based on user tier/preferences
   - Jobs are enqueued to the job queue (BullMQ)
   - Agents pick up jobs independently from the queue
   - Jobs include dependencies (e.g., Analysis job depends on data being fresh)

4. **Pipeline Orchestration** (agents execute independently):

   - **Analysis Agent Job**: Enqueued with ticker and analysis types
     - Agent picks up job from queue independently
     - Agent reads collected data from database
     - Agent performs analysis and writes results to database
     - Agent completes job and marks status in database
   - **Content Generation Agent Job**: Enqueued after Analysis (with dependency)
     - Agent picks up job from queue independently
     - Agent reads analysis results and user preferences from database
     - Agent generates newsletter and writes to database
     - Agent completes job and marks status in database
     - **Note**: Content generation runs sequentially per user-ticker subscription to ensure proper personalization. Multiple user-ticker subscriptions can be processed in parallel by different queue workers.
   - **Quality Assurance Agent Job**: Enqueued after Content Generation (with dependency)
     - Agent picks up job from queue independently
     - Agent reads newsletter from database
     - Agent validates content and writes approval status to database
     - If not approved, writes `requiresRevision: true` to database
     - Scheduler (or separate retry mechanism) can enqueue Content Generation retry job based on QA results
     - Note: QA Agent does not directly trigger retries; it only writes approval status to database
   - **Delivery Agent Job**: Enqueued after QA approval
     - Agent picks up job from queue independently
     - Agent reads approved newsletter from database
     - Agent sends email and writes metrics to database
     - Agent completes job and marks status in database

5. **Independent Agent Execution**:

   - Agents do not receive data from other agents
   - Agents read from and write to database independently
   - Agents execute on their own schedule when jobs are available
   - No direct agent-to-agent communication
   - All coordination through database state

6. **Job Dependencies & Status Tracking**:

   - Track job status in database (pending, running, completed, failed)
   - Use job dependencies to ensure proper sequencing
   - Monitor pipeline progress by querying database for job statuses
   - Handle failed jobs with retry logic (exponential backoff)

7. **Parallelization**:

   - Multiple user-ticker subscriptions can be processed in parallel
   - Analysis jobs for different tickers run in parallel
   - Content generation runs sequentially per user (to ensure proper personalization and avoid conflicts)
   - Multiple users can have their content generated in parallel (different queue workers)
   - Delivery jobs run in parallel (multiple recipients can receive newsletters simultaneously)

8. **Error Handling**:

   - Failed jobs are retried with exponential backoff (configured in `retry` section)
   - Job dependencies can timeout if upstream jobs fail repeatedly
   - Log errors to database for monitoring
   - Notify on critical failures (via configured notification channels)
   - Dead-letter queue for jobs that exceed max retries
   - Pipeline continues with partial results if some jobs fail (non-blocking)
   - Failed newsletter generation for one user-ticker doesn't block others

9. **Metrics Collection**:

   - Agents write metrics to database upon completion
   - Scheduler queries database for pipeline status
   - Learning Agent reads metrics from database independently (runs daily at midnight)

10. **Return**: Pipeline orchestration results (job IDs, status, metadata)

## Sequence Diagram

<Mermaid
  chart="
flowchart TD
    A[Cron Trigger / Manual] --> B[Scheduler Agent]
    B --> C[Load user-ticker subscriptions]
    C --> D[Check data freshness<br/>in Database]
    D --> D1{Data Fresh?}
    D1 -->|No<br/>> 4 hours old| D2[Enqueue Data Collection Job<br/>Agent picks up independently]
    D1 -->|Yes| E[Enqueue Pipeline Jobs<br/>per user-ticker]
    D2 --> E
    
    E --> E1[Enqueue Analysis Job<br/>to Queue]
    E1 --> E2[Enqueue Content Gen Job<br/>with dependency]
    E2 --> E3[Enqueue QA Job<br/>with dependency]
    E3 --> E4[Enqueue Delivery Job<br/>with dependency]
    
    subgraph Queue [Job Queue - BullMQ]
        Q1[Analysis Job]
        Q2[Content Gen Job]
        Q3[QA Job]
        Q4[Delivery Job]
    end
    
    E1 --> Q1
    E2 --> Q2
    E3 --> Q3
    E4 --> Q4
    
    subgraph Agents [Agents Execute Independently]
        direction TB
        A1[Analysis Agent<br/>Picks up job from queue<br/>Reads data from DB<br/>Writes analysis to DB<br/>Marks job complete]
        A2[Content Gen Agent<br/>Picks up job from queue<br/>Reads analysis from DB<br/>Reads user prefs from DB<br/>Writes newsletter to DB<br/>Marks job complete]
        A3[QA Agent<br/>Picks up job from queue<br/>Reads newsletter from DB<br/>Writes approval to DB<br/>Marks job complete]
        A4[Delivery Agent<br/>Picks up job from queue<br/>Reads approved newsletter from DB<br/>Sends email<br/>Writes metrics to DB<br/>Marks job complete]
    end
    
    Q1 --> A1
    Q2 --> A2
    Q3 --> A3
    Q4 --> A4
    
    A3 --> A3B{Approved?}
    A3B -->|No| A3C[Enqueue Content Gen<br/>Retry Job]
    A3B -->|Yes| A4
    A3C --> Q2
    
    A1 --> DB[(Database<br/>All agents read/write<br/>independently)]
    A2 --> DB
    A3 --> DB
    A4 --> DB
    
    DB --> F[Scheduler queries DB<br/>for pipeline status]
    F --> G[Learning Agent reads metrics<br/>from DB independently<br/>Daily at midnight]
    G --> H[Return orchestration results]
"
/>
